## Arraylist

- 基于数组，默认大小是10

- 扩容为原来的1.5倍，还要复制到新数组，代价大，因此最好指定初始大小，为了避免频繁扩容也可以自己手动调用ensureCapacity方法来手动增加ArrayList实例的容量

- 删除元素并把后边的数据顺次往前移动一位，代价高

- ArrayList序列化

  >ArrayList的序列化不太一样，它使用`transient`修饰存储元素的`elementData`的数组，`transient`关键字的作用是让被修饰的成员属性不被序列化。
  >
  >**为什么最ArrayList不直接序列化元素数组呢？**
  >
  >出于效率的考虑，数组可能长度100，但实际只用了50，剩下的50不用其实不用序列化，这样可以提高序列化和反序列化的效率，还可以节省内存空间。
  >
  >**那ArrayList怎么序列化呢？**
  >
  >ArrayList通过两个方法readObject、writeObject自定义序列化和反序列化策略，实际直接使用两个流`ObjectOutputStream`和`ObjectInputStream`来进行序列化和反序列化。只序列化有值的数据。

- Fail-Fast

  >modCount用来记录数组的扩容、增加、删除，在迭代等操作时，开始以前记录一个modCount,结束后查看有没有变化，有的话就抛出异常
  >
  >java.util包下的集合类都是快速失败的，不能在多线程下发生并发修改（迭代过程中被修改）

## CopyOnWriteArrayList

- 写操作是复制，然后指向新的，写时加锁
- 用于读多写少，内存占用大，数据同步不及时不适合实时性要求高的场景

## LinkedList

- Node(前后双指针，元素)
- 既可以看做容器，又可以看做队列，同时可以看做一个栈

## HashMap

- 允许null，null没有hashcode放在固定的位置

- 使用链表的尾插法，先插入再扩容，1.8中链表大于8时转为红黑树（当链表长度大于等于8且数组长度大于等于64，链表就会转化为红黑树。如果只满足一个条件，那么会优先选择数组扩容），小于6时又恢复为原来的链表（是因为如果这个阈值也设置成8，假如发生碰撞，节点增减刚好在8附近，会发生链表和红黑树的不断转换，导致资源浪费）

  >黑树本质上是一种二叉查找树，为了保持平衡，它又在二叉查找树的基础上增加了一些规则：
  >
  >1. 每个节点要么是红色，要么是黑色；
  >2. 根节点永远是黑色的；
  >3. 所有的叶子节点都是是黑色的（注意这里说叶子节点其实是图中的 NULL 节点）；
  >4. 每个红色节点的两个子节点一定都是黑色；
  >5. 从任一节点到其子树中每个叶子节点的路径都包含相同数量的黑色节点；
  >
  >为什么不用二叉树与平衡二叉树：
  >
  >红黑树是一种平衡的二叉树，插入、删除、查找的最坏时间复杂度都为 O(logn)，避免了二叉树最坏情况下的O(n)时间复杂度。
  >
  >平衡二叉树是比红黑树更严格的平衡树，为了保持保持平衡，需要旋转的次数更多，也就是说平衡二叉树保持平衡的效率更低，所以平衡二叉树插入和删除的效率比红黑树要低。

- 什么是扰动函数

  >HashMap的哈希函数是先拿到 key 的hashcode，是一个32位的int类型的数值，然后让hashcode的高16位和低16位进行异或操作。这么设计是为了降低哈希碰撞的概率。
  >
  >扰动函数之所以可以降低哈希碰撞概率是因为单纯的哈希码对数组长度取余以后当数组长度很短时，高位其实没参数运算（因为用的与代替了取余），为了混合原始哈希码的高位和低位，以此来加大低位的随机性。而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来。

- 为什么hashmap的容量是2的倍数

  >1. 方便哈希取余，将%运算转为&运算（因为对2^n-1取余与进行%运算结果是一样的）
  >2. 扩容时转移数据更方便，要么不用移动，要么只需要原索引+以前桶的容量

- 扩容，根据总数据数（N）调整桶数（M）,扩容时调整为原来的两倍并且需要重hash原来的值，比较费时

## ConcurrentHashMap

- 在jdk1.8中使用的数据结构与HashMap一样，保证线程安全的实现为synchronized + CAS + Unsafe，jdk1.8中锁的粒度变的更细，能支持的最大并发度为table数组的长度，并发度比1.7有所提高，jdk1.8能通过多个线程扩容。

- put的执行过程

  >1. 根据key的hashCode()计算hash值
  >
  >2. 如果table数组为空，则初始化table数组
  >
  >3. 定位到的Node为null时，利用CAS尝试放入，成功则退出，失败则再此进入循环
  >
  >4. 如果当前位置的Node.hash等于-1，则ConcurrentHashMap正在扩容，当前线程去帮助转移
  >
  >   ```
  >   因为红黑树的结点为TreeBin，hash值为-2。还有一个特殊节点ForwardingNode（表示数组正在扩容），hash值为-1
  >   ```
  >
  >5. 利用synchronized加锁，将元素放入红黑树或者链
  >
  >6. 如果链表的长度>=8，并且数组长度>=64，转为红黑树，否则扩容

## LinkedHashMap

- 继承自hashmap，内部维护了一个双向链表

- accessOrder为true时节点被访问就会移到链表尾部，这个值默认为false

- put以后如果removeEldestEntry()为ture时就会移除首节点

- LinkedHashMap实现的是插入的有序

  >根据以上特性，用LinkkedHashMap实现一个LRU时，要使得accessOrder为true并且重写removeEldestEntry方法
  >
  >```java
  >class LRUCache<K, V> extends LinkedHashMap<K, V> {
  > private final int maxCapacity;
  >
  > protected boolean removeEldestEntry(Map.Entry eldest) {
  >    return size() > MAX_ENTRIES;
  > }
  >
  > public LRUCache(int maxCapacity) {
  >    super(maxCapacity, 0.75f, true);
  >    this.maxCapacity=maxCapacity;
  > }
  >
  > public Collection<Map.Entry<K, V>> getAll() {
  >          return new ArrayList<>(super.entrySet());
  > }
  >}
  >```
  >
  >较为复杂的实现，利用hashmap+双链表实现
  >
  >```java
  >public class LRUCache {
  >    class DLinkedNode {
  >        int key;
  >        int value;
  >        DLinkedNode prev;
  >        DLinkedNode next;
  >        public DLinkedNode() {}
  >        public DLinkedNode(int _key, int _value) {key = _key; value = _value;}
  >    }
  >
  >    private Map<Integer, DLinkedNode> cache = new HashMap<Integer, DLinkedNode>();
  >    private int size;
  >    private int capacity;
  >    private DLinkedNode head, tail;
  >
  >    public LRUCache(int capacity) {
  >        this.size = 0;
  >        this.capacity = capacity;
  >        // 使用伪头部和伪尾部节点
  >        head = new DLinkedNode();
  >        tail = new DLinkedNode();
  >        head.next = tail;
  >        tail.prev = head;
  >    }
  >
  >    public int get(int key) {
  >        DLinkedNode node = cache.get(key);
  >        if (node == null) {
  >            return -1;
  >        }
  >        // 如果 key 存在，先通过哈希表定位，再移到头部
  >        moveToHead(node);
  >        return node.value;
  >    }
  >
  >    public void put(int key, int value) {
  >        DLinkedNode node = cache.get(key);
  >        if (node == null) {
  >            // 如果 key 不存在，创建一个新的节点
  >            DLinkedNode newNode = new DLinkedNode(key, value);
  >            // 添加进哈希表
  >            cache.put(key, newNode);
  >            // 添加至双向链表的头部
  >            addToHead(newNode);
  >            ++size;
  >            if (size > capacity) {
  >                // 如果超出容量，删除双向链表的尾部节点
  >                DLinkedNode tail = removeTail();
  >                // 删除哈希表中对应的项
  >                cache.remove(tail.key);
  >                --size;
  >            }
  >        }
  >        else {
  >            // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部
  >            node.value = value;
  >            moveToHead(node);
  >        }
  >    }
  >
  >    private void addToHead(DLinkedNode node) {
  >        node.prev = head;
  >        node.next = head.next;
  >        head.next.prev = node;
  >        head.next = node;
  >    }
  >
  >    private void removeNode(DLinkedNode node) {
  >        node.prev.next = node.next;
  >        node.next.prev = node.prev;
  >    }
  >
  >    private void moveToHead(DLinkedNode node) {
  >        removeNode(node);
  >        addToHead(node);
  >    }
  >
  >    private DLinkedNode removeTail() {
  >        DLinkedNode res = tail.prev;
  >        removeNode(res);
  >        return res;
  >    }
  >}
  >```

## WeakHashMap

>Tomcat 中的 ConcurrentCache 使用了 WeakHashMap 来实现缓存功能。
>
>ConcurrentCache 采取的是分代缓存：
>
>- 经常使用的对象放入 eden 中，eden 使用 ConcurrentHashMap 实现，不用担心会被回收（伊甸园）；
>- 不常用的对象放入 longterm，longterm 使用 WeakHashMap 实现，这些老对象会被垃圾收集器回收。
>- 当调用 get() 方法时，会先从 eden 区获取，如果没有找到的话再到 longterm 获取，当从 longterm 获取到就把对象放入 eden 中，从而保证经常被访问的节点不容易被回收。
>- 当调用 put() 方法时，如果 eden 的大小超过了 size，那么就将 eden 中的所有对象都放入 longterm 中，利用虚拟机回收掉一部分不经常使用的对象。

## PriorityQueue

- 使用堆结构实现

## TreeMap

- TreeMap 是按照 Key 的自然顺序或者 Comprator 的顺序进行排序，内部是通过红黑树来实现。所以要么 key 所属的类实现 Comparable 接口，或者自定义一个实现了 Comparator 接口的比较器，传给 TreeMap 用于 key 的比较。
- TreeMap的有序是键值有序


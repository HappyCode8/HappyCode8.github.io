## 为什么要用kafka

> 解耦、异步、削峰

## kafka的基本概念

>生产者（producer）
>
>消费者（consumer）
>
>消费者组（consumer group）：对于同一个topic，会广播给不同的group,一个group中，只有一个consumer可以消费该信息。
>
>broker：kafak集群中每个kafka的节点
>
>broker组：按照broker进行分组，同一个partition不会被多个broker同时保存。当一个partiiton非常大的时候，可以通过多个broker同时保存，但不会被保存多份。
>
>主题（topic）：一类消息
>
>分区（partion）：数据存储的基本单元。一个topic数据会被分散到多个partition,每一个partition都是有序的。消费者数目<= partition的数目
>
>集群（cluster）：broker的组合
>
>副本（repliaction）：基本单位是partition，所有读和写都从leader进，follower只做备份，且follower必须能够及时复制keader的数据；
>
>可用副本（AR）：分区中所有副本,ISR+OR
>
>同步中的副本（ISR，In-Sync Replicas）：所有与leader副本保持一定程度同步的副本，超过一段时间未复制数据就会从ISR中移除
>
>同步外的副本（OSR，Out-Sync Replicas）：与leader副本滞后过多的副本
>
>高水位（HW）:消费者只能拉取这个offset之前的消息
>
>日志尾端：（LEO，log end offset）：下一条待写入消息的offset，ISR中每一个都有自己的LEO，最小的LEO为分区的HW

## kafka的消费语义

>至少一次(at-least-once): 关闭offset自动提交，消费完了但是没发确认，开机重消费，至少不会丢数据，客户端可以自己去重。
>
>至多一次(at-most-once): 开启offset自动提交，下一次再消费的时候就会认为offset已经成功了，直接丢弃消息。就会造成丢数据。
>
>仅一次(exactly-once): 开启offset自动提交，可以开启consumer.seek()方法，相当于自己处理分区和offset，可以在此基础上开启事务，保持原子性，只有数据库保存成功再提交offset，保证两者同时成功。

## offset的自动提交与手动提交

>默认自动提交，可以通过enable.auto.commit配置，auto.commit.interval.ms与其配合使用，指定提交偏移量的频率，默认5秒。值太小增加网络流量，太大故障时会收到大量重复数据。
>
>有同步、异步之分，无参的方法适用于所有订阅的主题和分区，带参的只会提交Map中指定的偏移量、分区和主题。使用手工提交的好处是可以直接控制记录何时被视为已处理，防止拉下来但是没实际处理完的情况。

## 如何优化Kafka的写入速度

>1. 增加线程
>2. 提高 batch.size
>3. 增加更多 producer 实例
>4. 增加 partition 数
>5. 设置 acks=-1 时，如果延迟增大：可以增大 num.replica.fetchers（follower 同步数据的线程数）来调解
>6. 跨数据中心的传输：增加 socket 缓冲区设置以及 OS tcp 缓冲区设置。

## ack 为 0， 1， -1 各自的含义？

>1. 1（默认） 数据发送到Kafka后，经过leader成功接收消息的的确认，就算是发送成功了。在这种情况下，如果leader宕机了，则会丢失数据。
>2. 0 生产者将数据发送出去就不管了，不去等待任何返回。这种情况下数据传输效率最高，但是数据可靠性确是最低的。
>3. -1 producer需要等待ISR中的所有follower都确认接收到数据后才算一次发送完成，可靠性最高。当ISR中所有Replica都向Leader发送ACK时，leader才commit，这时候producer才能认为一个请求中的消息都commit了。

## kafka unclean 配置

>unclean.leader.election.enable 为true的话，意味着非ISR集合的broker 也可以参与选举，这样有可能就会丢数据，spark streaming在消费过程中拿到的 end offset 会突然变小，导致 spark streaming job挂掉。如果unclean.leader.election.enable参数设置为true，就有可能发生数据丢失和数据不一致的情况，Kafka的可靠性就会降低；而如果unclean.leader.election.enable参数设置为false，Kafka的可用性就会降低。

## leader crash时，ISR为空怎么办

>kafka在Broker端提供了一个配置参数：unclean.leader.election,这个参数有两个值：
>
>true（默认）：允许不同步副本成为leader，由于不同步副本的消息较为滞后，此时成为leader，可能会出现消息不一致的情况。
>
>false：不允许不同步副本成为leader，此时如果发生ISR列表为空，会一直等待旧leader恢复，降低了可用性。

## kafka的message格式是什么样的

>Message=固定长度的header+变长的消息体body
>
>header=一个字节的magic(文件格式)+四个字节的CRC32(用于判断body消息体是否正常)
>
>当magic=1时，会在magic和crc32之间多一个字节的数据：attributes(保存一些相关属性，比如是否压缩、压缩格式等等);如果magic的值为0，那么不存在attributes属性
>
>body是由N个字节构成的一个消息体，包含了具体的key/value消息

## Kafka为什么这么快

> - 并行处理
>
>   一方面，由于不同 Partition 可位于不同机器，因此可以充分利用集群优势，实现机器间的并行处理。另一方面，由于 Partition 在物理上对应一个文件夹，即使多个 Partition 位于同一个节点，也可通过配置让同一节点上的不同 Partition 置于不同的磁盘上，从而实现磁盘间的并行处理，充分发挥多磁盘的优势。
>
> - 顺序写
>
>   **Kafka 中每个分区是一个有序的，不可变的消息序列**，新的消息不断追加到 partition 的末尾，这个就是顺序写。
>
> - 利用了现代操作系统分页存储 Page Cache 来利用内存提高 I/O 效率
>
>   Broker 收到数据后，写磁盘时只是将数据写入 Page Cache，并不保证数据一定完全写入磁盘。从这一点看，可能会造成机器宕机时，Page Cache 内的数据未写入磁盘从而造成数据丢失。但是这种丢失只发生在机器断电等造成操作系统不工作的场景，而这种场景完全可以由 Kafka 层面的 Replication 机制去解决。如果为了保证这种情况下数据不丢失而强制将 Page Cache 中的数据 Flush 到磁盘，反而会降低性能。也正因如此，Kafka 虽然提供了 `flush.messages` 和 `flush.ms` 两个参数将 Page Cache 中的数据强制 Flush 到磁盘，但是 Kafka 并不建议使用。
>
> - 零拷贝
>
>   1. 网络数据持久化到磁盘 (Producer 到 Broker)
>   2. 磁盘文件通过网络发送（Broker 到 Consumer)
>
> - 批处理
>
>   Kafka 的客户端和 broker 还会在通过网络发送数据之前，在一个批处理中累积多条记录 (包括读和写)。记录的批处理分摊了网络往返的开销，使用了更大的数据包从而提高了带宽利用率。
>
> - 数据压缩
>
>   Producer 可将数据压缩后发送给 broker，从而减少网络传输代价，目前支持的压缩算法有：Snappy、Gzip、LZ4。数据压缩一般都是和批处理配套使用来作为优化手段的
>   
> - 文件分段

## Kafka中的消息是否会丢失和重复消费？

> 生产时：
>
> ack=0，网络、缓存区满了可能会丢
>
> ack=1，leader宕机而没有及时同步到副本会丢
>
> 消费时：
>
> 如果使用高级接口High-level API，可能存在一个问题就是当消息消费者从集群中把消息取出来、并提交了新的消息offset值后，还没来得及消费就挂掉了，那么下次再消费时之前没消费成功的消息就“诡异”的消失了；

## 消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?

> offset+1

## kafka是pull还是push？

> Pull
>
> push:像Scripe和apache flume是使用push 模式。优点：broker能以最大速率发送消息。缺点：当broker推送的消息远大于消费者的速率时，消费者就会崩溃，当broker想避免消费者崩溃，采取远小于消费者速率推送消息，导致一次推送较小的消息造成浪费。
>
> pull:消费者主动向broker拉取消息，适合不同消费速率的消费者。pull缺点：当broker没有消息推送时，导致消费者不断等待轮询。为了避免kafaka有个参数，可以以让消费者阻塞。

## ZK在kafka中的作用

> ZooKeeper 管理者所有的 Topic 和 Partition。
>
> Topic 和 Partition 存储在 Node 物理节点中，ZooKeeper负责维护这些 Node。Topic A 的 Partition #1 有3份，分布在各个 Node 上。这样可以增加 Kafka 的可靠性和系统弹性。3个 Partition #1 中，ZooKeeper 会指定一个 Leader，负责接收生产者发来的消息。其他2个 Partition #1 会作为 Follower，Leader 接收到的消息会复制给 Follower。这样，每个 Partition 都含有了全量消息数据。即使某个 Node 节点出现了故障，也不用担心消息的损坏。

## kafka的rebalance

>触发时机：
>
>1. 消费者数量变化
>
>2. 分区数量变化（只允许增大）
>
>3. 主题创建
>
>   当消费者订阅主题时使用的是正则表达式，例如 “test.*”，表示订阅所有以 test 开头的主题，当有新的以 test 开头的主题被创建时，则需要通过再均衡将该主题的分区分配给消费者。
>
>平衡策略：
>
>1. Round Robin：会采用轮询的方式将当前所有的分区依次分配给所有的consumer；
>2. Range：首先会计算每个consumer可以消费的分区个数，然后按照顺序将指定个数范围的分区分配给各个consumer；
>3. Sticky：这种分区策略是最新版本中新增的一种策略，其主要实现了两个目的：
>   -- 将现有的分区尽可能均衡的分配给各个consumer，存在此目的的原因在于Round Robin和Range分配策略实际上都会导致某几个consumer承载过多的分区，从而导致消费压力不均衡；
>   -- 如果发生再平衡，那么在重新分配前的基础上会尽力保证当前未宕机的consumer所消费的分区不会被分配给其他的consumer上；

## 待处理笔记：

- Kafka是一个消息代理

- Kafka将消息存储在主题中，并从主题检索消息。消息的生产者和消费者之间不会直接连接。此外，Kafka并不会保持有关生产者和消费者的任何状态，它仅作为一个消息交换中心。主题可以视为按名称分隔的日志。

- Kafka是一个日志

  Kafka主题底层的技术是日志，它是Kafka追加输入记录的文件。kafka的日志是一种只能追加、完全按照时间顺序排列的记录序列。

- Kafka日志工作原理

  当安装Kafka时，其中一个配置项是log.dir，该配置项用来指定Kafka存储日志数据的路径。每个主题都映射到指定日志路径下的一个子目录。子目录数与主题对应的分区数相同，目录名格式为“主题名_分区编号”。每个目录里面存放的都是用于追加传入消息的日志文件，一旦日志文件达到某个规模（磁盘上的记录总数或者记录的大小），或者消息的时间戳间的时间间隔达到了所配置的时间间隔时，日志文件就会被切分，传入的消息将会被追加到一个新的日志文件中

  >logs
  >
  >logs/topicA_0   A有1个分许
  >
  >logs/topicB_0   B有3个分区
  >
  >logs/topicB_1
  >
  >logs/topicB_2

- Kafka和分区

  分区是Kafka设计的一个重要部分，它对性能来说必不可少。分区保证了同一个键的数据将会按序被发送给同一个消费者。

  对主题作分区的本质是将发送到主题的数据切分到多个平行流之中，这是Kafka能够实现巨大吞吐量的关键。我们解释过每个主题就是一个分布式日志，每个分区类似于一个它自己的日志，并遵循相同的规则。Kafka将每个传入的消息追加到日志末尾，并且所有的消息都严格按时间顺序排列，每条消息都有一个分配给它的偏移量。Kafka不保证跨分区的消息有序，但是能够保证每个分区内的消息是有序的。

  除了增加吞吐量，分区还有另一个目的，它允许主题的消息分散在多台机器上，这样给定主题的容量就不会局限于一台服务器上的可用磁盘空间。

- 分区按键对数据进行分组

  Kafka处理键/值对格式的数据，如果键为空，那么生产者将采用轮询（round-robin）方式选择分区写入记录。如果键不为空，Kafka会使用以下公式（如下伪代码所示）确定将键/值对发送到哪个分区。通过使用确定性方法来选择分区，使得具有相同键的记录将会按序总是被发送到同一个分区。默认的分区器使用此方法，如果需要使用不同的策略选择分区，则可以提供自定义的分区器。

- 自定义分区器

  组合键分区识就需要自定义，可以使用下面配置指定一个自定义分区器

  partitioner.class=bbejeck_2.partitioner.PurchaseKeyPartitioner

- 确定恰当的分区数

  在创建主题时决定要使用的分区数既是一门艺术也是一门科学。其中一个重要的考虑因素是流入该主题的数据量。更多的数据意味着更多的分区以获得更高的吞吐量，但与生活中的任何事物一样，也要有取舍。

  增加分区数的同时也增加了TCP连接数和打开的文件句柄数。此外，消费者处理传入记录所花费的时间也会影响吞吐量。如果消费者线程有重量级处理操作，那么增加分区数可能有帮助，但是较慢的处理操作最终将会影响性能。

- 分布式日志

  当对主题进行分区时，Kafka不会将这些分区分布在一台服务上，而是将分区分散到集群中的多台服务器上。由于Kafka是在日志中追加记录，因此Kafka通过分区将这些记录分发到多台服务器上。

- 控制器

  Kafka使用ZooKeeper来选择代理控制器，如果代理控制器发生故障或者由于任何原因而不可用时，ZooKeeper从与领导者保持同步的一系列代理（已同步的副本[ISR]）中选出一个新的控制器。控制器的职责是为一个主题的所有分区建立领导者分区和追随者分区的关系。

- 日志管理-日志删除

  当一条新消息到达时，如果它的时间戳大于日志中第一个消息的时间戳加上log.roll.ms配置项配置的值时，Kafka就会切分日志。此时，日志被切分，一个新的日志段会被创建并作为一个活跃的日志段，而以前的活跃日志段仍然为消费者提供消息检索，日志切分有两个可选的配置项。

  log.roll.ms——这个是主配置项，但没有默认值。
  log.roll.hours——这是辅助配置项，仅当log.roll.ms没有被设置时使用，该配置项默认值是168小时。

  与日志切分一样，日志段的删除也基于消息的时间戳，而不仅是时钟时间或文件最后被修改的时间，日志段的删除基于日志中最大的时间戳。用来设置日志段删除策略的3个配置项按优先级依次列出如下，这里按优先级排列意味着排在前面的配置项会覆盖后面的配置项。

  log.retention.ms——以毫秒（ms）为单位保留日志文件的时长。
  log.retention.minutes——以分钟（min）为单位保留日志文件的时长。
  log.retention.hours——以小时（h）为单位保留日志文件。

  提出这些设置的前提是基于大容量主题的假设，这里大容量是指在一个给定的时间段内保证能够达到文件最大值。另一个配置项log.retention.bytes，可以指定较长的切分时间阈值，以控制I/O操作。最后，为了防止日志切分阈值设置得相对较大而出现日志量显著增加的情况，请使用配置项log.segment.bytes来控制单个日志段的大小。

  对于键为空的记录以及独立的记录[12]，删除日志的效果很好。但是，如果消息有键并需要预期的更新操作，那么还有一种方法更适合。

- 日志管理-日志压缩

  log.cleanup.policy=compact 根据日志的键值进行数据的更新，压缩支持按照主题配置

  删除操作会为给定键设置一个null值，作为一个墓碑标记。任何值为null的键都确保先前与其键相同的记录被去除，之后墓碑标记自身也会被去除。

- 生产者发送消息

  ```java
  Properties properties = new Properties();
  //后边可以使用一个逗号分隔的列表
  properties.put("bootstrap.servers", "localhost:9092");
  //KV序列化器
  properties.put("key.serializer","org.apache.kafka.common.serialization.StringSerializer");
  properties.put("value.serializer","org.apache.kafka.common.serialization.StringSerializer");
  //all:需要代理接收到所有追随者都以提交记录的确认
  //1:不等待追随者
  //0:不等待任何确认
  properties.put("acks", "1");
  //重试：如果记录的顺讯很重要，需要设置max.in.flight.requests.connect=1，以防止失败的记录在重试发送之前第二批记录成功发送
  properties.put("retries", "3");
  //数据压缩
  properties.put("compression.type", "snappy");
  //分区器类
  properties.put("partitioner.class",PurchaseKeyPartitioner.class.getName());　　
  PurchaseKey key = new PurchaseKey("12334568", new Date());
  
  try(Producer<PurchaseKey, String> producer = new KafkaProducer<>(properties)) {
     ProducerRecord<PurchaseKey, String> record = new ProducerRecord<>("transactions", key, {\"item\":\"book\",\"price\":10.99}");　　
     Callback callback = (metadata, exception) -> {
               if (exception != null) {
                  System.out.println("Encountered exception " + exception); 　　
              }
         };
      //生产者线程安全，一旦生产者将记录放到内部缓冲区，就立即返回Producer.send。缓冲区批量发送记录，具体取决于配置，如果在生产者缓冲区满时尝试发送消息，则可能会有阻塞。这里描述的Producer.send方法接受一个Callback实例，一旦领导者代理确认收到记录，生产者就会触发Callback.onComplete方法                                                                                   
      Future<RecordMetadata> sendFuture = producer.send(record, callback); 　
  }
  ```

- 指定分区

  希望所有分区收到的数据量大致相同

  ```java
  AtomicInteger partitionIndex = new AtomicInteger(0);  　　
  int currentPartition = Math.abs(partitionIndex.getAndIncrement())%numberPartitions; 
  ProducerRecord<String, String> record =  new ProducerRecord<>("topic", currentPartition, "key", "value");”
  ```

- 时间戳

  如果ProducerRecord对象设置了时间戳，那就使用这个时间戳

  如果主题设置了时间戳优先用主题的，否则使用代理级别的

  代理级别的log.message.timestamp.type配置可以被设置为CreateTime和LogAppendTime中的LogAppendTime被认为是“处理时间”，而CreateTime被认为是“事件时间。

- 消费者读取消息

  偏移量唯一标识消息，并表示消息在日志中的起始位置。消费者需要周期性地提交它们已接收到的消息的偏移量。提交一是完全处理了消息，二是发生故障或者重启时该消费者消费的起始位置。故障后消费者从何处开始消费消息取决于具体的配置。auto.offset.reset=

    - earliest从最早可用偏移量开始检索
    - latest从最新的偏移量开始检索，本质上是加入时间带你开始消费
    - none不指定重置策略，代理将会向消费者抛出异常

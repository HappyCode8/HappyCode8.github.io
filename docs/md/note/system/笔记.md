# 服务注册与发现-选择CP还是AP

>https://blog.csdn.net/qq40988670/article/details/105966202
>
>CP架构：分区异常时阻塞服务直到分区问题解决，保证一致性，常见的CP场景是对数据一致性特别敏感的业务，比如支付、HBase等分布式数据库，出现网络异常时就暂停服务，ZK也是优先保证CP的。否则银行大量取现、数据库随机返回新老数据都会有一系列问题。
>
>AP架构：数据不一致时，返回新老不同的数据，互联网常见，微博系统多地部署，微信朋友圈等，12306购票系统反复看到某车次有余票，但购买时却没有，相比CP用户体验更好
>
>```
>基于ZK的服务发现(CP)
>1. 在ZK中创建一个根路径，可以以接口命名，在这个接口下再创建调用与提供方的目录（比如providers、consumers）
>2. 提供方注册时，在提供方目录创建临时节点存储服务提供方的注册信息(临时节点是因为临时节点的生命周期与客户端会话相关，一旦机器出故障临时节点会被自动从Zookeeper删除)
>3. 订阅时，在调用目录创建一个临时节点，同时watch该服务提供方目录的所有节点
>4. 当提供方目录下有节点数据变更时，Zk通知给发起订阅的服务方
>
>问题1：
>ZK的特点是强一致性，ZK节点数据每次更新都通知其他节点同时执行更新操作，当连到ZK的节点数量特别多对ZK读写频繁且ZK存储目录达到一定数量时，ZK将不再稳定，CPU持续升高，最终宕机。宕机后，由于各业务节点还在持续发送读写请求，刚一启动就因为无法承接瞬间读写压力，马上宕机。
>
>问题2：
>ZooKeeper 无法正确处理服务发现的网络分区。在 ZooKeeper 中，无法达到仲裁数量的分区的节点客户端完全无法与ZooKeeper 及其服务发现机制进行通信。
>```
>
>```
>基于Eureka的服务发现(AP)
>1、Eureka-Client 在初始化时会将服务实例信息注册到任意一个 Eureka-Server，并且每隔 30 秒发送心跳请求。
>2、该 Eureka-Server 会将注册、心跳的请求，批量打包同步到其他 Eureka-Server。
>
>问题1：
>订阅端拿到的是服务的全量的地址：这个对于客户端的内存是一个比较大的消耗，特别在多数据中心部署的情况下，某个数据中心的订阅端往往只需要同数据中心的服务提供端即可。
>
>问题2：
>客户端采用周期性向服务端主动 pull 服务数据的模式（也就是客户端轮训的方式），这个方式存在实时性不足以及无谓的拉取性能消耗的问题。
>
>问题3：
>Eureka 集群的多副本的一致性协议采用类似“异步多写”的 AP 协议，每一个 server 都会把本地接收到的写请求发送给组成集群的其他所有的机器（Eureka 称之为 peer），特别是 hearbeat 报文是周期性持续不断的在 client->server->all peers 之间传送；这样的一致性算法，导致了如下问题
>
>- 每一台Server都需要存储全量的服务数据，Server 的内存明显会成为瓶颈。
>- 当订阅者却来越多的时候，需要扩容 Eureka 集群来提高读的能力，但是扩容的同时会导致每台 server  需要承担更多的写请求，扩容的效果不明显。
>- 组成 Eureka 集群的所有 server 都需要采用相同的物理配置，并且只能通过不断的提高配置来容纳更多的服务数据。
>
>扩展：Eureka 2.0, 为了解决上述问题而提出的，主要包含了如下的改进和增强：
>数据推送从 pull 走向 push 模式，并且实现更小粒度的服务地址按需订阅的功能。
>读写分离：写集群相对稳定，无需经常扩容；读集群可以按需扩容以提高数据推送能力。
>新增审计日志的功能和功能更丰富的 Dashboard。
>```
>
># 总结
>
>对于服务发现而言，拥有可能包含虚假信息的信息要比根本不拥有任何信息更好，所以个人认为 AP 优于 CP。
>
>在 AP 模式下，如果请求到不准确的服务实例信息，导致请求发送到一个宕机的服务端，只要做好失败重试机制和负载均衡，这次请求能够顺利的进行。

